{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_hAJHAAH4m"
      },
      "outputs": [],
      "source": [
        "! pip -q install transformers wandb pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PeB-WdBUJzL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT0wyhub3wuw"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "def set_seed(seed: int = 42, set_torch=True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    if set_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aRkkcRjtkDM"
      },
      "source": [
        "# Test Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwcEWeJkV6sy"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-small', padding_side='left')\n",
        "# model = AutoModelForCausalLM.from_pretrained('tinkoff-ai/ruDialoGPT-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqt1c-tCWjAq"
      },
      "outputs": [],
      "source": [
        "# chat_history_ids = \"\"\n",
        "\n",
        "# for step in range(5):\n",
        "#     chat_history_ids = chat_history_ids + \"@@ПЕРВЫЙ@@ \" + input(\">> User: \") + \"@@ВТОРОЙ@@\"\n",
        "#     new_input_ids = tokenizer(chat_history_ids, return_tensors='pt')\n",
        "#     generated_token_ids = model.generate(\n",
        "#         **new_input_ids,\n",
        "#         top_k=10,\n",
        "#         top_p=0.95,\n",
        "#         num_beams=3,\n",
        "#         num_return_sequences=1,\n",
        "#         do_sample=True,\n",
        "#         no_repeat_ngram_size=2,\n",
        "#         temperature=1.7,\n",
        "#         repetition_penalty=1.2,\n",
        "#         length_penalty=1.0,\n",
        "#         eos_token_id=50257,\n",
        "#         max_new_tokens=40,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "\n",
        "#     context_with_response = tokenizer.decode(generated_token_ids[0])\n",
        "#     cutted_answer = context_with_response[len(chat_history_ids):]\n",
        "#     if \"@@ПЕРВЫЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ПЕРВЫЙ@@\")[0]\n",
        "#     if \"@@ВТОРОЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ВТОРОЙ@@\")[0]\n",
        "#     chat_history_ids = chat_history_ids + cutted_answer\n",
        "#     print(f\"ruDialoGPT: \", cutted_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7Eehnq0ud0"
      },
      "source": [
        "# Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "512SZhXn0vmn"
      },
      "outputs": [],
      "source": [
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.seed = 42\n",
        "        self.word_dropout = 0\n",
        "        self.batch_size = 2\n",
        "        self.val_size = 0.2\n",
        "        self.learning_rate = 1e-4\n",
        "        # self.txt_in_min_len = 2\n",
        "        # self.txt_in_max_len = 8\n",
        "        # self.txt_out_min_len = 4\n",
        "        # self.txt_out_max_len = 16\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.max_epochs = 5\n",
        "        self.total_steps = int(13832 * self.max_epochs)\n",
        "        self.warmup_steps = int(0.05 * self.total_steps)\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AfYNNEttmtp"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-GahihFdtX_s",
        "outputId": "d6646f02-f00a-4ec3-893b-085ae55d0c28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d0ed60aa-e6cd-4f16-a7ed-068a95a5f28f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_3</th>\n",
              "      <th>context_2</th>\n",
              "      <th>context_1</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5828</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>в clearml можно тегами версионировать и фильтр...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6729</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>А зеркально или параллельно учить на русском и...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12126</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Очень легко сделать симуляцию в GTA5, использу...</td>\n",
              "      <td>Есть какой-нибудь гайд как это сделать? Уже да...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2974</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Хм, осталось 4 реакции</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2308</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>привет всем! \\n\\nя Сюзанна, \\nr&amp;d в RobotMIA, ...</td>\n",
              "      <td>А что делаете в RobotMIA если не секрет?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0ed60aa-e6cd-4f16-a7ed-068a95a5f28f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d0ed60aa-e6cd-4f16-a7ed-068a95a5f28f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d0ed60aa-e6cd-4f16-a7ed-068a95a5f28f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b9c5aa2-fa1e-43f9-816d-861e309588c2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b9c5aa2-fa1e-43f9-816d-861e309588c2')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b9c5aa2-fa1e-43f9-816d-861e309588c2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      context_3 context_2                                          context_1  \\\n",
              "5828        NaN       NaN                                                NaN   \n",
              "6729        NaN       NaN                                                NaN   \n",
              "12126       NaN       NaN  Очень легко сделать симуляцию в GTA5, использу...   \n",
              "2974        NaN       NaN                                                NaN   \n",
              "2308        NaN       NaN  привет всем! \\n\\nя Сюзанна, \\nr&d в RobotMIA, ...   \n",
              "\n",
              "                                                response  \n",
              "5828   в clearml можно тегами версионировать и фильтр...  \n",
              "6729   А зеркально или параллельно учить на русском и...  \n",
              "12126  Есть какой-нибудь гайд как это сделать? Уже да...  \n",
              "2974                              Хм, осталось 4 реакции  \n",
              "2308            А что делаете в RobotMIA если не секрет?  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/data2.csv\")\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne5GFJ-HimWU",
        "outputId": "7d630918-98be-4a0d-d8cb-f4700ead9edf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13832, 4)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCa98Lg9t00z"
      },
      "outputs": [],
      "source": [
        "# data_clear = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "s3xmQKRlwj9a",
        "outputId": "cce71088-04e8-4285-fa52-bc52098559e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d48927ae-30c0-4dbc-9cbc-7c2d129bc40d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_3</th>\n",
              "      <th>context_2</th>\n",
              "      <th>context_1</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7164</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Да одна нозологическая группа\\nИнтересно</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7553</th>\n",
              "      <td></td>\n",
              "      <td>может смотреть на количество слогов в строчке?...</td>\n",
              "      <td>может не рифмоваться же</td>\n",
              "      <td>там же разные варианты есть и они конечны. каж...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td></td>\n",
              "      <td>Чат для нейронок в области биологии, химии, фи...</td>\n",
              "      <td>Ага уже давно белки и хим связи с помощью Grap...</td>\n",
              "      <td>В молекулах генерацию на графах не особо делаю...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d48927ae-30c0-4dbc-9cbc-7c2d129bc40d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d48927ae-30c0-4dbc-9cbc-7c2d129bc40d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d48927ae-30c0-4dbc-9cbc-7c2d129bc40d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     context_3                                          context_2  \\\n",
              "7164                                                                \n",
              "7553            может смотреть на количество слогов в строчке?...   \n",
              "573             Чат для нейронок в области биологии, химии, фи...   \n",
              "\n",
              "                                              context_1  \\\n",
              "7164                                                      \n",
              "7553                            может не рифмоваться же   \n",
              "573   Ага уже давно белки и хим связи с помощью Grap...   \n",
              "\n",
              "                                               response  \n",
              "7164           Да одна нозологическая группа\\nИнтересно  \n",
              "7553  там же разные варианты есть и они конечны. каж...  \n",
              "573   В молекулах генерацию на графах не особо делаю...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_filled = data.fillna(\"\")\n",
        "data_filled.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVkydX2vRq20"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3FXJs7M3zLO"
      },
      "outputs": [],
      "source": [
        "class ApplyWordDropout:\n",
        "    def __init__(self, replace_with, eos_token_id, word_dropout=0.0):\n",
        "        self.keep_prop = 1.0 - word_dropout\n",
        "        self.replace_with = replace_with\n",
        "        self.eos_token_id = eos_token_id\n",
        "\n",
        "    def _apply_word_dropout(self, tensor):\n",
        "        dropout_mask = torch.rand(tensor.shape) < self.keep_prop\n",
        "        dropout_mask &= tensor != self.eos_token_id\n",
        "        result = torch.where(dropout_mask, tensor, torch.full_like(tensor, self.replace_with))\n",
        "        return result\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return self._apply_word_dropout(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO4ypTacxEc7"
      },
      "outputs": [],
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, df, cfg):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            'tinkoff-ai/ruDialoGPT-small',\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.word_dropout = ApplyWordDropout(\n",
        "            replace_with=self.tokenizer(self.tokenizer.unk_token)['input_ids'][0],\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            word_dropout=cfg.word_dropout,\n",
        "        )\n",
        "        self.samples = []\n",
        "        for _, sentences in df.iterrows():\n",
        "            conv = self._concat_conv(sentences, self.tokenizer)\n",
        "            self.samples.append(conv)\n",
        "        if cfg.word_dropout:\n",
        "            self.samples = [self.word_dropout(sample) for sample in self.samples]\n",
        "\n",
        "    def _concat_conv(self, sentences, tokenizer):\n",
        "        eos_list = [50257, 50258, 50257, 50258, 50257]\n",
        "        conv = [\n",
        "            torch.cat(\n",
        "                (\n",
        "                    torch.tensor([eos_list.pop()]).unsqueeze(0),\n",
        "                    tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"],\n",
        "                ),\n",
        "                dim=1,\n",
        "            )\n",
        "            for sentence in sentences\n",
        "            if sentence != \"\"\n",
        "        ]\n",
        "        conv[-1] = torch.cat(\n",
        "            (\n",
        "                conv[-1],\n",
        "                torch.tensor([eos_list.pop()]).unsqueeze(0),\n",
        "            ),\n",
        "            dim=1,\n",
        "        )\n",
        "        conv_flat = torch.cat(conv, dim=1).view(-1)\n",
        "        return conv_flat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.samples[item].to(torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZwQvQ02Kdv5"
      },
      "outputs": [],
      "source": [
        "class ConversationDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, cfg):\n",
        "        super().__init__()\n",
        "        train_data, val_data = train_test_split(data, test_size=cfg.val_size)\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = ConversationDataset(self.train_data, self.cfg)\n",
        "        self.val_dataset = ConversationDataset(self.val_data, self.cfg)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.cfg.batch_size, shuffle=True, collate_fn=self._collate)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.cfg.batch_size, collate_fn=self._collate)\n",
        "\n",
        "    # def _collate(self, examples: list[torch.Tensor]):\n",
        "    #     return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    def _collate(self, examples: list[torch.Tensor]):\n",
        "        max_length = max([len(ex) for ex in examples])\n",
        "        padded_examples = [F.pad(ex, (max_length - len(ex), 0)) for ex in examples]\n",
        "        return torch.stack(padded_examples, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5he3VGVdPcQu"
      },
      "outputs": [],
      "source": [
        "class DialoTuner(pl.LightningModule):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"tinkoff-ai/ruDialoGPT-small\")\n",
        "        self.cfg = cfg\n",
        "        self.perplexity = torchmetrics.text.Perplexity()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        inputs, labels = (batch, batch)\n",
        "        return self.model(input, labels=labels)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.cfg.learning_rate)\n",
        "        # Calculate the total number of training steps\n",
        "        total_steps = self.cfg.total_steps\n",
        "        # Create the scheduler with linear warmup and decay\n",
        "        lr_scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=self.cfg.warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        inputs, labels = (train_batch, train_batch)\n",
        "        outputs = self.model(inputs, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        inputs, labels = (val_batch, val_batch)\n",
        "        outputs = self.model(inputs, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        perplexity_score = self.perplexity(outputs.logits, labels)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.log(\"val_perplexity\", perplexity_score)\n",
        "        return loss\n",
        "\n",
        "    def generate(self, **kwargs):\n",
        "        return self.model.generate(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashbi3W151Ef"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEYohKWGVjx1"
      },
      "outputs": [],
      "source": [
        "# conversation_data_module = ConversationDataModule(data_filled, cfg)\n",
        "# model = DialoTuner(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhvTy17w29tR"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# key = \"5d9fffc78b7b93676008c4a6a8ac3b28a11dd421\"\n",
        "# wandb.login(key=key)\n",
        "\n",
        "# config_dict = {attr: getattr(cfg, attr) for attr in dir(cfg) if not callable(getattr(cfg, attr)) and not attr.startswith(\"__\")}\n",
        "# logger = pl.loggers.WandbLogger(project='sandbox', config=config_dict, log_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4NYFZ7v55CZ"
      },
      "outputs": [],
      "source": [
        "# trainer = pl.Trainer(\n",
        "#     accelerator=\"gpu\",\n",
        "#     devices=[0],\n",
        "#     max_epochs=cfg.max_epochs,\n",
        "#     logger=logger,\n",
        "#     log_every_n_steps=1,\n",
        "#     gradient_clip_val=1.0,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF3YAIad694m"
      },
      "outputs": [],
      "source": [
        "# trainer.fit(model, datamodule=conversation_data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCf54ygZDfVd"
      },
      "outputs": [],
      "source": [
        "# trainer.validate(model, datamodule=conversation_data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiOQaVvKNxt1"
      },
      "outputs": [],
      "source": [
        "# torch.save(model, 'finetuned_model_10ep_1e4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On2L8sDKNapb"
      },
      "outputs": [],
      "source": [
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypVbiB9-bfF5"
      },
      "source": [
        "# Simple Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ_fg5YCC98Y"
      },
      "outputs": [],
      "source": [
        "# chat_history_ids = \"\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#             'tinkoff-ai/ruDialoGPT-small',\n",
        "#             padding_side='left'\n",
        "#         )\n",
        "\n",
        "\n",
        "# for step in range(5):\n",
        "#     chat_history_ids = chat_history_ids + \"@@ПЕРВЫЙ@@ \" + input(\">> User: \") + \"@@ВТОРОЙ@@\"\n",
        "#     new_input_ids = tokenizer(chat_history_ids, return_tensors='pt')\n",
        "#     generated_token_ids = model.generate(\n",
        "#         **new_input_ids,\n",
        "#         top_k=10,\n",
        "#         top_p=0.95,\n",
        "#         num_beams=3,\n",
        "#         num_return_sequences=1,\n",
        "#         do_sample=True,\n",
        "#         no_repeat_ngram_size=2,\n",
        "#         temperature=1.7,\n",
        "#         repetition_penalty=1.2,\n",
        "#         length_penalty=1.0,\n",
        "#         eos_token_id=50257,\n",
        "#         max_new_tokens=40,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "\n",
        "#     context_with_response = tokenizer.decode(generated_token_ids[0])\n",
        "#     cutted_answer = context_with_response[len(chat_history_ids):]\n",
        "#     if \"@@ПЕРВЫЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ПЕРВЫЙ@@\")[0]\n",
        "#     if \"@@ВТОРОЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ВТОРОЙ@@\")[0]\n",
        "#     chat_history_ids = chat_history_ids + cutted_answer\n",
        "#     print(f\"ruDialoGPT: \", cutted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBh8wmLgDw8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simxnpGabqG_"
      },
      "source": [
        "# LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRTERO5abeOg"
      },
      "outputs": [],
      "source": [
        "!pip install \"peft\" --quiet\n",
        "!pip install \"accelerate\" \"evaluate\" loralib --upgrade --quiet\n",
        "!pip install accelerate -U --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnC8gWBacbHx",
        "outputId": "184569c1-2085-4418-b6db-1b0b04f9efb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-small', padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained('tinkoff-ai/ruDialoGPT-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INnPYRd4bxoD",
        "outputId": "8325dfa6-b331-467d-ff23-19232ca406a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,265,856 || all params: 128,495,168 || trainable%: 2.5416177517274425\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "#  target_modules=[\"q\", \"v\"],\n",
        "    target_modules=[\"wte\", \"lm_head\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# model = prepare_model_for_int8_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBm0nWcifgTm"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Since you're doing regular language modeling, set mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6MmQh_-i2vM"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    optim=\"adamw_torch\",\n",
        "    output_dir=\"./output\",\n",
        "    report_to =\"wandb\",\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=3000,\n",
        "    overwrite_output_dir=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=1,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    prediction_loss_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUbXnDItlIiu",
        "outputId": "22d80513-6074-45e6-ade1-b655df127592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_LOG_MODEL=true\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwhiteroseraf\u001b[0m (\u001b[33mhackathon-fu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%env WANDB_LOG_MODEL=true\n",
        "\n",
        "key = \"5d9fffc78b7b93676008c4a6a8ac3b28a11dd421\"\n",
        "wandb.login(key=key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Hihi_PrVz7"
      },
      "outputs": [],
      "source": [
        "perplexity_metric = torchmetrics.text.Perplexity()\n",
        "\n",
        "def compute_perplexity(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    perplexity = perplexity_metric(predictions, labels)\n",
        "    return {\"perplexity\": perplexity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7a3ip8AjHgI",
        "outputId": "f7421035-094c-4bb4-fdf8-74980aca864b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "conversation_data_module = ConversationDataModule(data_filled, cfg)\n",
        "conversation_data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGhC4pvpsga9"
      },
      "outputs": [],
      "source": [
        "model = model.to(cfg.device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=conversation_data_module.train_dataset,\n",
        "    eval_dataset=conversation_data_module.val_dataset,\n",
        "    # compute_metrics=compute_perplexity,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "REZQ99i1jiey",
        "outputId": "aab95da3-b53c-4b8e-8c6f-2f0d567f5ff0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230825_231715-q1nabjj3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hackathon-fu/huggingface/runs/q1nabjj3' target=\"_blank\">earthy-wind-9</a></strong> to <a href='https://wandb.ai/hackathon-fu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/hackathon-fu/huggingface' target=\"_blank\">https://wandb.ai/hackathon-fu/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/hackathon-fu/huggingface/runs/q1nabjj3' target=\"_blank\">https://wandb.ai/hackathon-fu/huggingface/runs/q1nabjj3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13830' max='13830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13830/13830 28:20, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.544000</td>\n",
              "      <td>4.579616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.370800</td>\n",
              "      <td>4.465733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.297500</td>\n",
              "      <td>4.414749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>4.241900</td>\n",
              "      <td>4.391315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13830, training_loss=4.443358155289855, metrics={'train_runtime': 1701.8915, 'train_samples_per_second': 32.508, 'train_steps_per_second': 8.126, 'total_flos': 2889397055857536.0, 'train_loss': 4.443358155289855, 'epoch': 5.0})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "oKtzaR9xkG1b",
        "outputId": "dc7e737f-f2f6-4a42-e390-ef55a0a469bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2767' max='2767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2767/2767 00:59]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 4.387857437133789,\n",
              " 'eval_runtime': 59.1005,\n",
              " 'eval_samples_per_second': 46.819,\n",
              " 'eval_steps_per_second': 46.819,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523,
          "referenced_widgets": [
            "f822b77106c04cf09776c96319dc3367",
            "54eb7567ad29446288bf195bb184f2d6",
            "da76eff53ca94f508115e7a2ed2d164d",
            "e674a66a62c64de3b992fcd47d3d0aed",
            "1216ea28505648aab81c07052e40d722",
            "6493c335e05c4c78a441298e9b297624",
            "51bcc2241a9d4725bb4abf2e0bf379fd",
            "153acb0aceea407fbb2127e69da5e552"
          ]
        },
        "id": "xgZP9oUl-bg_",
        "outputId": "6f9af2b5-b071-4737-89b1-33053439fe75"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f822b77106c04cf09776c96319dc3367",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='12.483 MB of 12.484 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.9999…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▁▁</td></tr><tr><td>eval/runtime</td><td>▆█▁██</td></tr><tr><td>eval/samples_per_second</td><td>▃▁█▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▃▁█▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.38786</td></tr><tr><td>eval/runtime</td><td>59.1005</td></tr><tr><td>eval/samples_per_second</td><td>46.819</td></tr><tr><td>eval/steps_per_second</td><td>46.819</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>13830</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.2894</td></tr><tr><td>train/total_flos</td><td>2889397055857536.0</td></tr><tr><td>train/train_loss</td><td>4.44336</td></tr><tr><td>train/train_runtime</td><td>1701.8915</td></tr><tr><td>train/train_samples_per_second</td><td>32.508</td></tr><tr><td>train/train_steps_per_second</td><td>8.126</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earthy-wind-9</strong> at: <a href='https://wandb.ai/hackathon-fu/huggingface/runs/q1nabjj3' target=\"_blank\">https://wandb.ai/hackathon-fu/huggingface/runs/q1nabjj3</a><br/>Synced 4 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230825_231715-q1nabjj3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb0YfNLzi4qi"
      },
      "outputs": [],
      "source": [
        "path=\"lora-1ep\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csYKdMJ_GBuF"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNJ-7p_KGVkN"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(path + \"pretrained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU_0bDkqGuK-"
      },
      "outputs": [],
      "source": [
        "# model = model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "sAkmCD9-x6Tr",
        "outputId": "f5607108-f12f-4500-f17c-c5ee44ebffd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> User: на работу кто возьмет?\n",
            "ruDialoGPT:  Я бы взял, но у меня нет работы ((\n",
            ">> User: а данные есть?\n",
            "ruDialoGPT:  Есть конечно, просто я работаю на работе, и мне нужно их куда-нибудь девать. Но это не точно, я не уверен, что смогу это сделать в ближайшее время. 😁\n",
            ">> User: у гпт спроси\n",
            "ruDialoGPT:  У них есть датасеты в инете, можно посмотреть, если надо, то могу скинуть. Я не знаю, как их можно использовать в личных целях) Спасибо за инфу) Буду рад\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-41635de0f352>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"@@ПЕРВЫЙ@@ \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> User: \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"@@ВТОРОЙ@@\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnew_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     generated_token_ids = model.generate(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# chat_history_ids = \"\"\n",
        "\n",
        "# for step in range(5):\n",
        "#     chat_history_ids = chat_history_ids + \"@@ПЕРВЫЙ@@ \" + input(\">> User: \") + \"@@ВТОРОЙ@@\"\n",
        "#     new_input_ids = tokenizer(chat_history_ids, return_tensors='pt')\n",
        "#     generated_token_ids = model.generate(\n",
        "#         **new_input_ids,\n",
        "#         top_k=10,\n",
        "#         top_p=0.95,\n",
        "#         num_beams=3,\n",
        "#         num_return_sequences=1,\n",
        "#         do_sample=True,\n",
        "#         no_repeat_ngram_size=2,\n",
        "#         temperature=1.7,\n",
        "#         repetition_penalty=1.2,\n",
        "#         length_penalty=1.0,\n",
        "#         eos_token_id=50257,\n",
        "#         max_new_tokens=40,\n",
        "#         pad_token_id=tokenizer.eos_token_id\n",
        "#     )\n",
        "\n",
        "#     context_with_response = tokenizer.decode(generated_token_ids[0])\n",
        "#     cutted_answer = context_with_response[len(chat_history_ids):]\n",
        "#     if \"@@ПЕРВЫЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ПЕРВЫЙ@@\")[0]\n",
        "#     if \"@@ВТОРОЙ@@\" in cutted_answer:\n",
        "#         cutted_answer = cutted_answer.split(\"@@ВТОРОЙ@@\")[0]\n",
        "#     chat_history_ids = chat_history_ids + cutted_answer\n",
        "#     print(f\"ruDialoGPT: \", cutted_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J8QxZ65Gqfl",
        "outputId": "74fcddb0-5734-4e5b-fe8b-01eb904bcbb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@@ПЕРВЫЙ@@ где веса ламы найти?@@ВТОРОЙ@@в гугле вбивай датасеты и там найдёшь весы с ламой@@ПЕРВЫЙ@@<|endoftext|>',\n",
              " '@@ПЕРВЫЙ@@ где веса ламы найти?@@ВТОРОЙ@@в гугле вбивай датасеты и там найдёшь весы с ламами@@ПЕРВЫЙ@@<|endoftext|>',\n",
              " '@@ПЕРВЫЙ@@ где веса ламы найти?@@ВТОРОЙ@@в гугле вбивай датасеты и там найдёшь весы с ламой.@@ПЕРВЫЙ@@']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_str = \"где веса ламы найти?\"\n",
        "\n",
        "chat_history_ids = \"@@ПЕРВЫЙ@@ \" + input_str + \"@@ВТОРОЙ@@\"\n",
        "new_input_ids = tokenizer(chat_history_ids, return_tensors='pt')\n",
        "generated_token_ids = model.generate(\n",
        "        **new_input_ids,\n",
        "        top_k=10,\n",
        "        top_p=0.95,\n",
        "        num_beams=3,\n",
        "        num_return_sequences=3,\n",
        "        do_sample=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=1.4,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=1.0,\n",
        "        eos_token_id=50257,\n",
        "        max_new_tokens=40,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "context_with_response = [tokenizer.decode(generated_token_ids[i]) for i in range(len(generated_token_ids))]\n",
        "context_with_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWxMCP5nJlio"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1216ea28505648aab81c07052e40d722": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "153acb0aceea407fbb2127e69da5e552": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51bcc2241a9d4725bb4abf2e0bf379fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54eb7567ad29446288bf195bb184f2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1216ea28505648aab81c07052e40d722",
            "placeholder": "​",
            "style": "IPY_MODEL_6493c335e05c4c78a441298e9b297624",
            "value": "12.483 MB of 12.484 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "6493c335e05c4c78a441298e9b297624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da76eff53ca94f508115e7a2ed2d164d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51bcc2241a9d4725bb4abf2e0bf379fd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_153acb0aceea407fbb2127e69da5e552",
            "value": 0.9999583650083946
          }
        },
        "e674a66a62c64de3b992fcd47d3d0aed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f822b77106c04cf09776c96319dc3367": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54eb7567ad29446288bf195bb184f2d6",
              "IPY_MODEL_da76eff53ca94f508115e7a2ed2d164d"
            ],
            "layout": "IPY_MODEL_e674a66a62c64de3b992fcd47d3d0aed"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
