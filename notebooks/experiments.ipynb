{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/data_processed/data_processed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../data/data_processed/data_processed.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m data\u001b[39m.\u001b[39msample(\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data_processed/data_processed.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/processed/data_processed.csv\")\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3912, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_droped_nan = data.dropna(subset=['context_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2368, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_droped_nan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_3</th>\n",
       "      <th>context_2</th>\n",
       "      <th>context_1</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Если не подписывать, что это учебная дз практи...</td>\n",
       "      <td>Всем добрый денек! Занимаюсь задачей QA по кор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Или не сота, но хотя бы базированное решение, ...</td>\n",
       "      <td>Retrieve and rerank sbert, думаю</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Добрый день, с глупым вопросом возможно залечу...</td>\n",
       "      <td>А почему вы решили, что комната поменьше не жи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>opencv пока не пробовали, но можно.\\n\"Самый бо...</td>\n",
       "      <td>В вопросе была именно такая формулировка)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Всем привет! \\nПодскажите, пожалуйста, какие-т...</td>\n",
       "      <td>контекстов когда совсем не применим я не могу ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Всем привет, подскажите пожалуйста, как запуст...</td>\n",
       "      <td>Вынести в отдельный сервис, в котлине оставить...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Это нормально когда работодатель просит чтобы ...</td>\n",
       "      <td>Только если ты уже трудоустроен)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>звучит как скам)</td>\n",
       "      <td>Ладно</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>NaN</td>\n",
       "      <td>звучит как скам)</td>\n",
       "      <td>Ладно</td>\n",
       "      <td>дичь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>звучит как скам)</td>\n",
       "      <td>Ладно</td>\n",
       "      <td>дичь</td>\n",
       "      <td>кто ранил инференс whisper на multiple gpu? на...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2368 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             context_3         context_2  \\\n",
       "1                  NaN               NaN   \n",
       "3                  NaN               NaN   \n",
       "6                  NaN               NaN   \n",
       "9                  NaN               NaN   \n",
       "11                 NaN               NaN   \n",
       "...                ...               ...   \n",
       "3904               NaN               NaN   \n",
       "3907               NaN               NaN   \n",
       "3909               NaN               NaN   \n",
       "3910               NaN  звучит как скам)   \n",
       "3911  звучит как скам)             Ладно   \n",
       "\n",
       "                                              context_1  \\\n",
       "1     Если не подписывать, что это учебная дз практи...   \n",
       "3     Или не сота, но хотя бы базированное решение, ...   \n",
       "6     Добрый день, с глупым вопросом возможно залечу...   \n",
       "9     opencv пока не пробовали, но можно.\\n\"Самый бо...   \n",
       "11    Всем привет! \\nПодскажите, пожалуйста, какие-т...   \n",
       "...                                                 ...   \n",
       "3904  Всем привет, подскажите пожалуйста, как запуст...   \n",
       "3907  Это нормально когда работодатель просит чтобы ...   \n",
       "3909                                   звучит как скам)   \n",
       "3910                                              Ладно   \n",
       "3911                                               дичь   \n",
       "\n",
       "                                               response  \n",
       "1     Всем добрый денек! Занимаюсь задачей QA по кор...  \n",
       "3                      Retrieve and rerank sbert, думаю  \n",
       "6     А почему вы решили, что комната поменьше не жи...  \n",
       "9             В вопросе была именно такая формулировка)  \n",
       "11    контекстов когда совсем не применим я не могу ...  \n",
       "...                                                 ...  \n",
       "3904  Вынести в отдельный сервис, в котлине оставить...  \n",
       "3907                   Только если ты уже трудоустроен)  \n",
       "3909                                              Ладно  \n",
       "3910                                               дичь  \n",
       "3911  кто ранил инференс whisper на multiple gpu? на...  \n",
       "\n",
       "[2368 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_droped_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_3</th>\n",
       "      <th>context_2</th>\n",
       "      <th>context_1</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>По идее это может сделать стейбл диффужен с re...</td>\n",
       "      <td>вот да, видел примеры с стейбл, показалось, чт...</td>\n",
       "      <td>Смотря как подойти к делу, но вряд ли стоит вр...</td>\n",
       "      <td>мне кажется, это в первую очередь вопрос, как ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>вот да, видел примеры с стейбл, показалось, чт...</td>\n",
       "      <td>Смотря как подойти к делу, но вряд ли стоит вр...</td>\n",
       "      <td>мне кажется, это в первую очередь вопрос, как ...</td>\n",
       "      <td>Контролнет работает поверх клипа с тем на что ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Смотря как подойти к делу, но вряд ли стоит вр...</td>\n",
       "      <td>мне кажется, это в первую очередь вопрос, как ...</td>\n",
       "      <td>Контролнет работает поверх клипа с тем на что ...</td>\n",
       "      <td>А можешь че-то посоветовать по этой теме почит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>мне кажется, это в первую очередь вопрос, как ...</td>\n",
       "      <td>Контролнет работает поверх клипа с тем на что ...</td>\n",
       "      <td>А можешь че-то посоветовать по этой теме почит...</td>\n",
       "      <td>https://github.com/lllyasviel/ControlNet-v1-1-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Всем привет! \\nЗадача: классифкация на 277 кла...</td>\n",
       "      <td>В чем проблема с использованием стандартных мо...</td>\n",
       "      <td>вот кроме efficientnet в голову ничего и не пр...</td>\n",
       "      <td>Если надо сохранять размер картинки, то можно ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>https://arxiv.org/pdf/1904.02689.pdf</td>\n",
       "      <td>Пасиб за папиру, не до конца понял, как градие...</td>\n",
       "      <td>Предикт кропается по gt и по кропу считается лосс</td>\n",
       "      <td>Да, но как оно в коде трейна реализовано?) они...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>Пасиб за папиру, не до конца понял, как градие...</td>\n",
       "      <td>Предикт кропается по gt и по кропу считается лосс</td>\n",
       "      <td>Да, но как оно в коде трейна реализовано?) они...</td>\n",
       "      <td>не знаю что делают они, но чтобы бэкпроп не ло...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>Есть три типа сегментации:\\n\\nсемантическая (s...</td>\n",
       "      <td>Они пошли самым простым путём: заменили прямоу...</td>\n",
       "      <td>в U-Net можно заменить слой энкодера на любой ...</td>\n",
       "      <td>эт да, blip умеет\\nиногда даже слишком\\nно мне...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>Они пошли самым простым путём: заменили прямоу...</td>\n",
       "      <td>в U-Net можно заменить слой энкодера на любой ...</td>\n",
       "      <td>эт да, blip умеет\\nиногда даже слишком\\nно мне...</td>\n",
       "      <td>а что у BLIP'а там? Там нет эмбеддингов?\\nМне ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>звучит как скам)</td>\n",
       "      <td>Ладно</td>\n",
       "      <td>дичь</td>\n",
       "      <td>кто ранил инференс whisper на multiple gpu? на...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>541 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context_3  \\\n",
       "40    По идее это может сделать стейбл диффужен с re...   \n",
       "41    вот да, видел примеры с стейбл, показалось, чт...   \n",
       "42    Смотря как подойти к делу, но вряд ли стоит вр...   \n",
       "43    мне кажется, это в первую очередь вопрос, как ...   \n",
       "47    Всем привет! \\nЗадача: классифкация на 277 кла...   \n",
       "...                                                 ...   \n",
       "3889               https://arxiv.org/pdf/1904.02689.pdf   \n",
       "3890  Пасиб за папиру, не до конца понял, как градие...   \n",
       "3898  Есть три типа сегментации:\\n\\nсемантическая (s...   \n",
       "3899  Они пошли самым простым путём: заменили прямоу...   \n",
       "3911                                   звучит как скам)   \n",
       "\n",
       "                                              context_2  \\\n",
       "40    вот да, видел примеры с стейбл, показалось, чт...   \n",
       "41    Смотря как подойти к делу, но вряд ли стоит вр...   \n",
       "42    мне кажется, это в первую очередь вопрос, как ...   \n",
       "43    Контролнет работает поверх клипа с тем на что ...   \n",
       "47    В чем проблема с использованием стандартных мо...   \n",
       "...                                                 ...   \n",
       "3889  Пасиб за папиру, не до конца понял, как градие...   \n",
       "3890  Предикт кропается по gt и по кропу считается лосс   \n",
       "3898  Они пошли самым простым путём: заменили прямоу...   \n",
       "3899  в U-Net можно заменить слой энкодера на любой ...   \n",
       "3911                                              Ладно   \n",
       "\n",
       "                                              context_1  \\\n",
       "40    Смотря как подойти к делу, но вряд ли стоит вр...   \n",
       "41    мне кажется, это в первую очередь вопрос, как ...   \n",
       "42    Контролнет работает поверх клипа с тем на что ...   \n",
       "43    А можешь че-то посоветовать по этой теме почит...   \n",
       "47    вот кроме efficientnet в голову ничего и не пр...   \n",
       "...                                                 ...   \n",
       "3889  Предикт кропается по gt и по кропу считается лосс   \n",
       "3890  Да, но как оно в коде трейна реализовано?) они...   \n",
       "3898  в U-Net можно заменить слой энкодера на любой ...   \n",
       "3899  эт да, blip умеет\\nиногда даже слишком\\nно мне...   \n",
       "3911                                               дичь   \n",
       "\n",
       "                                               response  \n",
       "40    мне кажется, это в первую очередь вопрос, как ...  \n",
       "41    Контролнет работает поверх клипа с тем на что ...  \n",
       "42    А можешь че-то посоветовать по этой теме почит...  \n",
       "43    https://github.com/lllyasviel/ControlNet-v1-1-...  \n",
       "47    Если надо сохранять размер картинки, то можно ...  \n",
       "...                                                 ...  \n",
       "3889  Да, но как оно в коде трейна реализовано?) они...  \n",
       "3890  не знаю что делают они, но чтобы бэкпроп не ло...  \n",
       "3898  эт да, blip умеет\\nиногда даже слишком\\nно мне...  \n",
       "3899  а что у BLIP'а там? Там нет эмбеддингов?\\nМне ...  \n",
       "3911  кто ранил инференс whisper на multiple gpu? на...  \n",
       "\n",
       "[541 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/lora-5ep\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"привет\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50257 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'привет@@ВТОРОЙ@@Привет, как дела? 🥺@@ПЕРВЫЙ@@'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(input_str, return_tensors=\"pt\")\n",
    "generated_token_ids = model.generate(\n",
    "            **input_ids,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            num_beams=3,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=1.5,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=1.0,\n",
    "            eos_token_id=50257,\n",
    "            max_new_tokens=40,\n",
    "        )\n",
    "generated_output = tokenizer.decode(generated_token_ids[0], skip_special_tokens=True)\n",
    "cutted_answer = generated_output\n",
    "# cut off the answer if it contains the special tokens\n",
    "# if \"@@ПЕРВЫЙ@@\" in cutted_answer:\n",
    "#     cutted_answer = cutted_answer.split(\"@@ПЕРВЫЙ@@\")[0]\n",
    "# if \"@@ВТОРОЙ@@\" in cutted_answer:\n",
    "#     cutted_answer = cutted_answer.split(\"@@ВТОРОЙ@@\")[0]\n",
    "\n",
    "cutted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@@ПЕРВЫЙ@@ где веса ламы найти?@@ВТОРОЙ@@В гугле по картинкам.@@ПЕРВЫЙ@@']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"где веса ламы найти?\"\n",
    "\n",
    "chat_history_ids = \"@@ПЕРВЫЙ@@ \" + input_str + \"@@ВТОРОЙ@@\"\n",
    "new_input_ids = tokenizer(chat_history_ids, return_tensors='pt')\n",
    "generated_token_ids = model.generate(\n",
    "        **new_input_ids,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=1.4,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        eos_token_id=50257,\n",
    "        max_new_tokens=40,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "context_with_response = [tokenizer.decode(generated_token_ids[i]) for i in range(len(generated_token_ids))]\n",
    "context_with_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@@ПЕРВЫЙ@@ где найти веса ламы? @@ВТОРОЙ@@В интернете. Там много чего можно найти. Например, в гугле. Или в яндекс картинках.@@ПЕРВЫЙ@@']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-small')\n",
    "inputs = tokenizer('@@ПЕРВЫЙ@@ где найти веса ламы? @@ВТОРОЙ@@', return_tensors='pt')\n",
    "generated_token_ids = model.generate(\n",
    "    **inputs,\n",
    "    top_k=5,\n",
    "    top_p=0.9,\n",
    "    num_beams=3,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=1.7,\n",
    "    repetition_penalty=1.2,\n",
    "    length_penalty=1.0,\n",
    "    eos_token_id=50257,\n",
    "    max_new_tokens=40,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
    "context_with_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.28.2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting telebot\n",
      "  Using cached telebot-0.0.5-py3-none-any.whl (4.8 kB)\n",
      "Collecting pyTelegramBotAPI (from telebot)\n",
      "  Using cached pyTelegramBotAPI-4.13.0.tar.gz (232 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from telebot) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from requests->telebot) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from requests->telebot) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from requests->telebot) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from requests->telebot) (2022.12.7)\n",
      "Building wheels for collected packages: pyTelegramBotAPI\n",
      "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-4.13.0-py3-none-any.whl size=214546 sha256=42b335ca9dbe06e3b57c425bcf67dd685b28c9c9f6624720ac94fff15edef1c7\n",
      "  Stored in directory: /Users/macbook/Library/Caches/pip/wheels/6d/05/82/5f51ca10e3939eb47f61064a76b1fb027ff69927a25a3c5190\n",
      "Successfully built pyTelegramBotAPI\n",
      "Installing collected packages: pyTelegramBotAPI, telebot\n",
      "Successfully installed pyTelegramBotAPI-4.13.0 telebot-0.0.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
